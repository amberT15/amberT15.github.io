---
title:  "Evaluating the representational power of pre-trainedDNA language models for regulatory genomics"
excerpt: "BioRxiv, 2024"
layout: single
header:
  teaser: /assets/images/gLM.png
  url: https://www.biorxiv.org/content/10.1101/2024.02.29.582810v1
---

![Image from news and views.](/assets/images/gLM.png)

## Abstract:
The emergence of genomic language models (gLMs) offers an unsupervised approach to learn a wide diversity of cis-regulatory
patterns in the non-coding genome without requiring labels of functional activity generated by wet-lab experiments. Previous
evaluations have shown pre-trained gLMs can be leveraged to improve prediction performance across a broad range of
regulatory genomics tasks, albeit using relatively simple benchmark datasets and baseline models. Since the gLMs in these
studies were tested upon fine-tuning their weights for each downstream task, determining whether gLM representations embody
a foundational understanding of cis-regulatory biology remains an open question. Here we evaluate the representational power
of pre-trained gLMs to predict and interpret cell-type-specific functional genomics data that span DNA and RNA regulation.
Our findings suggest that current gLMs do not offer substantial advantages over conventional machine learning approaches
that use one-hot encoded sequences. This work highlights a major limitation with current gLMs, raising potential issues in
conventional pre-training strategies for the non-coding genome.

[Full Text](https://www.biorxiv.org/content/10.1101/2024.02.29.582810v1.full.pdf)
